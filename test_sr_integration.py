#!/usr/bin/env python3
"""Script ki·ªÉm tra t√≠ch h·ª£p MF-LPR Super-Resolution v√†o pipeline OCR.

Ch·∫°y script n√†y ƒë·ªÉ verify:
1. Module SR c√≥ import ƒë∆∞·ª£c kh√¥ng
2. Adapter MF_LPR_SR c√≥ kh·ªüi t·∫°o ƒë∆∞·ª£c kh√¥ng (n·∫øu c√≥ checkpoint)
3. Dataset c√≥ nh·∫≠n SR enhancer kh√¥ng
4. Pipeline c√≥ ch·∫°y ƒë∆∞·ª£c kh√¥ng (test v·ªõi 1 batch nh·ªè)

Usage:
    python test_sr_integration.py [--sr-checkpoint-path PATH] [--no-sr]
"""

from src.sr import MF_LPR_SR
from src.data.dataset import MultiFrameDataset
from configs.config import Config
import os
import sys
import torch
import argparse
from pathlib import Path

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


def test_sr_module_import():
    """Test 1: Ki·ªÉm tra module SR c√≥ import ƒë∆∞·ª£c kh√¥ng."""
    print("=" * 60)
    print("TEST 1: Ki·ªÉm tra import module SR")
    print("=" * 60)
    try:
        from src.sr import MF_LPR_SR
        print("‚úÖ PASS: Module `src.sr.MF_LPR_SR` import th√†nh c√¥ng")
        return True
    except ImportError as e:
        print(f"‚ùå FAIL: Kh√¥ng th·ªÉ import MF_LPR_SR. L·ªói: {e}")
        return False


def test_sr_adapter_init(checkpoint_path: str = None, config_path: str = None):
    """Test 2: Ki·ªÉm tra adapter SR c√≥ kh·ªüi t·∫°o ƒë∆∞·ª£c kh√¥ng."""
    print("\n" + "=" * 60)
    print("TEST 2: Ki·ªÉm tra kh·ªüi t·∫°o MF_LPR_SR adapter")
    print("=" * 60)

    if not checkpoint_path:
        print("‚ö†Ô∏è  SKIP: Kh√¥ng c√≥ checkpoint path, b·ªè qua test n√†y")
        print("   (B·∫°n c√≥ th·ªÉ test b·∫±ng c√°ch: --sr-checkpoint-path <path>)")
        return True

    if not os.path.exists(checkpoint_path):
        print(f"‚ùå FAIL: Checkpoint kh√¥ng t·ªìn t·∫°i: {checkpoint_path}")
        return False

    if config_path and not os.path.exists(config_path):
        print(f"‚ùå FAIL: Config kh√¥ng t·ªìn t·∫°i: {config_path}")
        return False

    try:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        sr_enhancer = MF_LPR_SR(
            checkpoint_path=checkpoint_path,
            config_path=config_path or "sr_model/config/LP-Diff.json",
            device=device,
        )
        print(f"‚úÖ PASS: MF_LPR_SR ƒë√£ kh·ªüi t·∫°o th√†nh c√¥ng tr√™n {device}")
        print(f"   - Checkpoint: {checkpoint_path}")
        print(f"   - Config: {config_path or 'sr_model/config/LP-Diff.json'}")
        return True, sr_enhancer
    except Exception as e:
        print(f"‚ùå FAIL: Kh√¥ng th·ªÉ kh·ªüi t·∫°o MF_LPR_SR. L·ªói: {e}")
        import traceback
        traceback.print_exc()
        return False, None


def test_sr_enhance_function(sr_enhancer):
    """Test 3: Ki·ªÉm tra h√†m enhance c√≥ ch·∫°y ƒë∆∞·ª£c kh√¥ng."""
    print("\n" + "=" * 60)
    print("TEST 3: Ki·ªÉm tra h√†m enhance_sequence")
    print("=" * 60)

    if sr_enhancer is None:
        print("‚ö†Ô∏è  SKIP: Kh√¥ng c√≥ SR enhancer, b·ªè qua test n√†y")
        return True

    try:
        # T·∫°o dummy input: (T=5, C=3, H=32, W=128), normalized [-1, 1]
        dummy_frames = torch.randn(5, 3, 32, 128) * 0.5  # Gi·∫£ l·∫≠p normalize
        print(f"   Input shape: {dummy_frames.shape}")

        with torch.no_grad():
            enhanced = sr_enhancer.enhance_sequence(
                dummy_frames,
                resize_to=(32, 128)
            )

        print(f"   Output shape: {enhanced.shape}")

        if enhanced.shape[0] == dummy_frames.shape[0]:
            print("‚úÖ PASS: enhance_sequence ho·∫°t ƒë·ªông ƒë√∫ng (gi·ªØ nguy√™n s·ªë frame)")
            return True
        else:
            print(
                f"‚ùå FAIL: S·ªë frame kh√¥ng kh·ªõp. Input: {dummy_frames.shape[0]}, Output: {enhanced.shape[0]}")
            return False
    except Exception as e:
        print(f"‚ùå FAIL: L·ªói khi ch·∫°y enhance_sequence. L·ªói: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_dataset_with_sr(sr_enhancer, data_root: str = None):
    """Test 4: Ki·ªÉm tra Dataset c√≥ nh·∫≠n SR enhancer kh√¥ng."""
    print("\n" + "=" * 60)
    print("TEST 4: Ki·ªÉm tra Dataset v·ªõi SR enhancer")
    print("=" * 60)

    if not data_root or not os.path.exists(data_root):
        print(f"‚ö†Ô∏è  SKIP: Data root kh√¥ng t·ªìn t·∫°i: {data_root}")
        print("   (B·∫°n c√≥ th·ªÉ test b·∫±ng c√°ch: --data-root <path>)")
        return True

    try:
        config = Config()
        config.CHAR2IDX = {char: idx + 1 for idx,
                           char in enumerate(config.CHARS)}

        # T·∫°o dataset v·ªõi SR enhancer
        dataset = MultiFrameDataset(
            root_dir=data_root,
            mode='train',
            img_height=config.IMG_HEIGHT,
            img_width=config.IMG_WIDTH,
            char2idx=config.CHAR2IDX,
            sr_enhancer=sr_enhancer,
            augmentation_level='light',  # D√πng light ƒë·ªÉ test nhanh
        )

        if len(dataset) == 0:
            print("‚ö†Ô∏è  WARNING: Dataset r·ªóng, kh√¥ng th·ªÉ test")
            return True

        # Test load 1 sample (MultiFrameDataset tr·∫£ v·ªÅ 7 tr∆∞·ªùng, l·∫•y 6 tr∆∞·ªùng ƒë·∫ßu)
        sample = dataset[0]
        images, targets, target_len, label, track_id, img_paths, _ = sample

        print(f"   Dataset size: {len(dataset)} samples")
        print(f"   Sample 0 - Images shape: {images.shape}")
        print(f"   Sample 0 - Label: {label}")
        print(f"   Sample 0 - Img paths: {img_paths}")

        if images.shape[0] == 5:  # 5 frames
            print("‚úÖ PASS: Dataset load ƒë∆∞·ª£c sample v·ªõi SR enhancer")
            if sr_enhancer is not None:
                print("   ‚úÖ SR enhancer ƒë√£ ƒë∆∞·ª£c g·∫Øn v√†o dataset")
            return True
        else:
            print(
                f"‚ùå FAIL: S·ªë frame kh√¥ng ƒë√∫ng. Expected: 5, Got: {images.shape[0]}")
            return False
    except Exception as e:
        print(f"‚ùå FAIL: L·ªói khi t·∫°o/load dataset. L·ªói: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_pipeline_integration(sr_enhancer, data_root: str = None):
    """Test 5: Ki·ªÉm tra pipeline ho√†n ch·ªânh (Dataset -> DataLoader -> Model input)."""
    print("\n" + "=" * 60)
    print("TEST 5: Ki·ªÉm tra pipeline ho√†n ch·ªânh")
    print("=" * 60)

    if not data_root or not os.path.exists(data_root):
        print(f"‚ö†Ô∏è  SKIP: Data root kh√¥ng t·ªìn t·∫°i: {data_root}")
        return True

    try:
        from torch.utils.data import DataLoader

        config = Config()
        config.CHAR2IDX = {char: idx + 1 for idx,
                           char in enumerate(config.CHARS)}

        dataset = MultiFrameDataset(
            root_dir=data_root,
            mode='train',
            img_height=config.IMG_HEIGHT,
            img_width=config.IMG_WIDTH,
            char2idx=config.CHAR2IDX,
            sr_enhancer=sr_enhancer,
            augmentation_level='light',
        )

        if len(dataset) == 0:
            print("‚ö†Ô∏è  SKIP: Dataset r·ªóng")
            return True

        loader = DataLoader(
            dataset,
            batch_size=2,
            shuffle=False,
            collate_fn=MultiFrameDataset.collate_fn,
            num_workers=0,  # D√πng 0 ƒë·ªÉ tr√°nh multiprocessing issues khi test
        )

        # Load 1 batch (7 tr∆∞·ªùng, b·ªè qua country_ids v√¨ ch·ªâ ki·ªÉm tra shape ·∫£nh)
        batch = next(iter(loader))
        images, targets, target_lengths, labels_text, track_ids, img_paths, _ = batch

        print(f"   Batch images shape: {images.shape}")  # (B, T=5, C, H, W)
        print(f"   Batch size: {images.shape[0]}")

        # Ki·ªÉm tra shape h·ª£p l·ªá cho model OCR
        if images.dim() == 5 and images.shape[1] == 5:
            print("‚úÖ PASS: Pipeline ho√†n ch·ªânh ho·∫°t ƒë·ªông ƒë√∫ng")
            print(
                f"   ‚úÖ Shape ph√π h·ª£p cho model OCR: (batch, frames=5, channels, height, width)")
            return True
        else:
            print(
                f"‚ùå FAIL: Shape kh√¥ng ƒë√∫ng. Expected: (B, 5, C, H, W), Got: {images.shape}")
            return False
    except Exception as e:
        print(f"‚ùå FAIL: L·ªói trong pipeline. L·ªói: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    parser = argparse.ArgumentParser(
        description="Ki·ªÉm tra t√≠ch h·ª£p MF-LPR Super-Resolution"
    )
    parser.add_argument(
        "--sr-checkpoint-path",
        type=str,
        default=None,
        help="ƒê∆∞·ªùng d·∫´n checkpoint SR ƒë·ªÉ test (n·∫øu c√≥)",
    )
    parser.add_argument(
        "--sr-config-path",
        type=str,
        default=None,
        help="ƒê∆∞·ªùng d·∫´n config JSON c·ªßa SR (m·∫∑c ƒë·ªãnh: sr_model/config/LP-Diff.json)",
    )
    parser.add_argument(
        "--data-root",
        type=str,
        default=None,
        help="ƒê∆∞·ªùng d·∫´n data ƒë·ªÉ test dataset (optional)",
    )
    parser.add_argument(
        "--no-sr",
        action="store_true",
        help="Test pipeline KH√îNG c√≥ SR (ƒë·ªÉ so s√°nh)",
    )

    args = parser.parse_args()

    print("\n" + "=" * 60)
    print("üîç KI·ªÇM TRA T√çCH H·ª¢P MF-LPR SUPER-RESOLUTION")
    print("=" * 60 + "\n")

    results = []

    # Test 1: Import
    results.append(("Import module", test_sr_module_import()))

    # Test 2: Init adapter
    sr_enhancer = None
    if not args.no_sr:
        if args.sr_checkpoint_path:
            success, sr_enhancer = test_sr_adapter_init(
                args.sr_checkpoint_path,
                args.sr_config_path
            )
            results.append(("Init SR adapter", success))
        else:
            print("\n‚ö†Ô∏è  Kh√¥ng c√≥ --sr-checkpoint-path, b·ªè qua test init adapter")
            results.append(("Init SR adapter", None))
    else:
        print("\n‚ö†Ô∏è  Flag --no-sr ƒë∆∞·ª£c set, b·ªè qua test SR")
        results.append(("Init SR adapter", None))

    # Test 3: Enhance function
    if sr_enhancer is not None:
        results.append(
            ("Enhance function", test_sr_enhance_function(sr_enhancer)))
    else:
        results.append(("Enhance function", None))

    # Test 4: Dataset integration
    data_root = args.data_root or getattr(Config(), "DATA_ROOT", None)
    results.append(
        ("Dataset integration", test_dataset_with_sr(sr_enhancer, data_root)))

    # Test 5: Pipeline integration
    results.append(
        ("Pipeline integration", test_pipeline_integration(sr_enhancer, data_root)))

    # T·ªïng k·∫øt
    print("\n" + "=" * 60)
    print("üìä T·ªîNG K·∫æT")
    print("=" * 60)

    passed = 0
    failed = 0
    skipped = 0

    for test_name, result in results:
        if result is None:
            status = "‚è≠Ô∏è  SKIPPED"
            skipped += 1
        elif result:
            status = "‚úÖ PASS"
            passed += 1
        else:
            status = "‚ùå FAIL"
            failed += 1
        print(f"   {test_name:30s} {status}")

    print(f"\n   T·ªïng: {passed} passed, {failed} failed, {skipped} skipped")

    if failed == 0:
        print("\n‚úÖ T·∫§T C·∫¢ TEST ƒê√É PASS! MF-LPR SR ƒë√£ ƒë∆∞·ª£c t√≠ch h·ª£p th√†nh c√¥ng.")
        return 0
    else:
        print(f"\n‚ùå C√ì {failed} TEST FAIL. Vui l√≤ng ki·ªÉm tra l·∫°i.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
